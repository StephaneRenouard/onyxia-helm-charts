vscode-python:
  # Nécessaire si Harbor est privé: crée le secret docker-registry côté cluster
  # puis référence-le ici (format Kubernetes standard).
  #
  # Exemple:
  # kubectl -n onyxia create secret docker-registry harbor-regcred \
  #   --docker-server=harbor.lan \
  #   --docker-username='robot$<name>' \
  #   --docker-password='<token>' \
  #   --docker-email='none'
  imagePullSecrets:
    - name: harbor-regcred

  ingress:
    enabled: false

  service:
    image:
      pullPolicy: IfNotPresent
      custom:
        enabled: true
        # Registry interne (Harbor)
        version: harbor.lan/premyom/onyxia-code-server:0.1.13

  extraEnvVars:
    - name: CODE_SERVER_AUTH
      value: none
    - name: VAULT_ADDR
      value: 'http://vault.vault.svc.cluster.local:8200'
    - name: VAULT_K8S_ROLE
      value: 'premyom-s3-read'
    - name: ONYXIA_USER_GROUPS
      value: '{{- $p := (default dict .premyom) -}}{{- $g := (default (list) $p.userGroupsJson) -}}{{- if kindIs "string" $g -}}{{- default "[]" $g -}}{{- else -}}{{- toJson $g -}}{{- end -}}'
    - name: PREMYOM_S3_MOUNT_ENABLED
      value: '{{- $p := (default dict .premyom) -}}{{- $s3 := (default dict $p.s3Mount) -}}{{- default "false" $s3.enabled -}}'
    - name: PREMYOM_S3_NONHDS_ENDPOINT_HOST
      value: '{{- $p := (default dict .premyom) -}}{{- $s3 := (default dict $p.s3Mount) -}}{{- default "" $s3.nonHdsEndpointHost -}}'
    - name: PREMYOM_S3_HDS_ENDPOINT_HOST
      value: '{{- $p := (default dict .premyom) -}}{{- $s3 := (default dict $p.s3Mount) -}}{{- default "" $s3.hdsEndpointHost -}}'
    - name: PREMYOM_S3_VAULT_NONHDS_PATH
      value: '{{- $p := (default dict .premyom) -}}{{- $s3 := (default dict $p.s3Mount) -}}{{- default "" $s3.vaultNonHdsPath -}}'
    - name: PREMYOM_S3_VAULT_HDS_PATH
      value: '{{- $p := (default dict .premyom) -}}{{- $s3 := (default dict $p.s3Mount) -}}{{- default "" $s3.vaultHdsPath -}}'
    - name: PREMYOM_S3_MOUNT_ROOT
      value: '{{- $p := (default dict .premyom) -}}{{- $s3 := (default dict $p.s3Mount) -}}{{- default "/mnt/s3" $s3.mountRoot -}}'

  # s3fs en pod Kubernetes nécessite FUSE: /dev/fuse + droits.
  # Pour la démo, on assume un pod privilégié.
  premyomS3fs:
    enabled: true

  securityContext:
    privileged: true
    allowPrivilegeEscalation: true
    runAsUser: 0
    runAsGroup: 0
    capabilities:
      add:
        - SYS_ADMIN

  persistence:
    size: 2Gi

  resources:
    limits:
      cpu: "2"
      memory: 4Gi
    requests:
      cpu: "1"
      memory: 2Gi

  # On désactive l'intégration Vault "upstream" (sinon le chart crée un secret Vault vide).
  # L'image `onyxia-code-server` récupère un token Vault via auth/kubernetes à l'exécution.
  vault:
    enabled: false
    secretName: ""
    token: ""
    url: ""
    mount: ""
    secret: ""
    directory: ""

  premyom:
    # Array attendu (x-onyxia: overwriteDefaultWith user.decodedIdToken.groups).
    # Le template convertit en JSON via `toJson`.
    userGroupsJson: []
    s3Mount:
      enabled: "true"
      nonHdsEndpointHost: "cellar-c2.services.clever-cloud.com"
      hdsEndpointHost: "cellar-c2.services.clever-cloud.com"
      vaultNonHdsPath: "secret/data/premyom/s3/nonhds"
      vaultHdsPath: "secret/data/premyom/s3/hds"
      mountRoot: "/mnt/s3"

sso:
  # SSO peut être géré de 2 façons :
  # - forwardAuth (recommandé) : on s'appuie sur le oauth2-proxy central (Ingress Traefik + middleware).
  #   => pas de redirect_uri par workspace, donc beaucoup plus robuste.
  # - embedded : déploie un oauth2-proxy par workspace (ancienne approche).
  mode: embedded

  forwardAuth:
    # Middleware Traefik appliqué à l'Ingress du workspace.
    # Référence KubernetesCRD: "<namespace>-<middleware>@kubernetescrd"
    middleware: onyxia-workspace-sso-chain@kubernetescrd

  secretName: oauth2-proxy
  issuerUrl: https://auth.datalab.arkam-group.com/auth/realms/onyxia
  # Si vide, le chart calcule un callback par workspace: https://<hostname>/oauth2/callback.
  # (requis pour le mode embedded, sinon risque de 403/CSRF mismatch)
  redirectUrl: ""
  # Si vide, on scope les cookies au hostname du workspace (recommandé).
  cookieDomain: ""
  whitelistDomain: ""
  ingress:
    enabled: true
    tls: true
    ingressClassName: ""
    annotations: {}
    hostname: chart-example.local
    path: /
